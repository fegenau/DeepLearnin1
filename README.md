# üß† Clasificaci√≥n de im√°genes con Deep Learning: Fashion MNIST

Este proyecto utiliza una red neuronal profunda para clasificar prendas de vestir del conjunto de datos **Fashion MNIST**. Se desarroll√≥ y entren√≥ el modelo en Google Colab utilizando TensorFlow/Keras. Adem√°s, se implementaron t√©cnicas de regularizaci√≥n, an√°lisis de hiperpar√°metros y visualizaci√≥n de resultados.

---

## üìÇ Contenido del Notebook

### 1. Carga de datos
- Se importa el dataset `Fashion MNIST` desde `keras.datasets`.
- Se dividen los datos en sets de entrenamiento y prueba.

### 2. Exploraci√≥n de datos
- Se visualizan algunas im√°genes del dataset.
- Se identifican las clases a predecir (camiseta, pantal√≥n, zapato, etc.).

### 3. Preprocesamiento
- Normalizaci√≥n de los pixeles (valores entre 0 y 1).
- Divisi√≥n adicional en set de validaci√≥n.

### 4. Creaci√≥n del modelo base
- Red neuronal simple con capas densas.
- Funci√≥n de activaci√≥n: `ReLU` y `Softmax`.

### 5. Entrenamiento y evaluaci√≥n
- Se entrena el modelo con los datos preprocesados.
- Se grafican las curvas de p√©rdida y precisi√≥n.
- Se eval√∫a el modelo en el set de prueba.

### 6. üõ°Ô∏è Regularizaci√≥n
- Se prueban distintas t√©cnicas para evitar el sobreajuste:
  - **Dropout**
  - **Batch Normalization**
  - **EarlyStopping**
- Se comparan los modelos con y sin regularizaci√≥n.

### 7. ‚öôÔ∏è Hiperpar√°metros
- Se ajustan y prueban:
  - `learning rate`
  - `batch size`
  - n√∫mero de `epochs`
- Se identifican combinaciones √≥ptimas.

### 8. üîç An√°lisis de predicciones
- Se visualizan im√°genes mal clasificadas.
- Se identifican patrones comunes en los errores.

---

## üìä Resultados

| Modelo                 | Precisi√≥n validaci√≥n |
|------------------------|----------------------|
| Modelo Base            | 0.87                 |
| Modelo con Dropout     | 0.88                 |
| Batch Normalization    | 0.89                 |
| Dropout + BatchNorm + EarlyStopping | **0.91** |

El mejor modelo fue aquel que combin√≥ m√∫ltiples t√©cnicas de regularizaci√≥n y par√≥ el entrenamiento en el momento √≥ptimo.

---

## üìå Conclusiones

- Las t√©cnicas de regularizaci√≥n permiten mejorar la **generalizaci√≥n** del modelo, reduciendo el sobreajuste.
- Un buen ajuste de **hiperpar√°metros** es crucial para obtener un rendimiento competitivo.
- El an√°lisis de errores permite entender las limitaciones del modelo y plantear mejoras futuras, como:
  - Uso de **redes convolucionales (CNN)**
  - **Data augmentation**
  - **Transfer learning**

---

## üöÄ Requisitos

- Python 3.x
- TensorFlow 2.x
- Google Colab (opcional pero recomendado)

---

## ‚úçÔ∏è Autor

Proyecto desarrollado como parte de una evaluaci√≥n parcial del curso de Deep Learning - Ingenier√≠a en Inform√°tica menci√≥n Ciencia de Datos.
